{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Imagem</h1>\n",
    "\n",
    "<h2>Kaggle</h2>\n",
    "<br>\n",
    "<lu>\n",
    "    <a href=\"https://www.kaggle.com/datamunge/sign-language-mnist\" target=\"_blank\">\n",
    "        <li>Sign Language MNIST</li>\n",
    "    </a>\n",
    "    <p>\n",
    "        Nessa base de dados temos um <b>arquivo csv com 27550 amostras</b>, ele não conta com a letra j e z, já que no sistema ASL são as únicas letras que demandam movimentação.<br>\n",
    "        Se parece muito mais útil para quem quer algum toyproblem para começar, pode ser especialmente útil para aprender.<br><br>\n",
    "    </p>\n",
    "    <a href=\"https://www.kaggle.com/grassknoted/asl-alphabet\" target=\"_blank\">\n",
    "        <li>ASL Alphabet</li>\n",
    "    </a>\n",
    "    <p>\n",
    "        A base de dados é composta por imagens(200x200) em jpg, com um total de <b>86990 amostras</b>.<br>\n",
    "        Conta com imagens para as letras com movimento, inclusive um fato muito interessante é contar com imagens adicionais para <b>espaço, delete e vazio</b> o que pode ser muito útil para criar certas lógicas na aplicação.<br>\n",
    "    </p>\n",
    "</lu>\n",
    "<h2>Academia</h2><br>\n",
    "<lu>\n",
    "    <a href = \"http://vlm1.uta.edu/~srujana/ASLID/ASL_Image_Dataset.html\" target = \"_blank\">\n",
    "        <li>American Sign Language Image Dataset</li>\n",
    "    </a>\n",
    "    <p>\n",
    "        Basicamente eles pegaram um outro dataset acadêmico e extrairam apenas suas imagens, com um total de <b>809 amostras(240 X 352)</b> para o training set e <b>479 amostras(480 X 640)</b> para o test set.\n",
    "    </p>\n",
    "</lu>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Vídeos</h1>\n",
    "\n",
    "<h2>Academia</h2>\n",
    "<lu>\n",
    "    <a href = \"http://www.bu.edu/av/asllrp/dai-asllvd.html\" target = \"_blank\">\n",
    "        <li>American Sign Language Lexicon Video Dataset (ASLLVD)</li>\n",
    "    </a>\n",
    "    <p> \n",
    "        É o melhor dataset em tese, com mais de <b>3300 classes diferentes, 9800 exemplos feitos por 6 pessoas diferentes</b>. O dataset está em formato de vídeo e conta com multiplos angulos, além de ter também o rosto, torso e braço das pessoas ao fazerem os movimentos.\n",
    "        Os dados são públicos com um peso de 1.2GB. Todos os detalhes a respeito do download e o formato dos vídeos podem ser encontrados <a href = \"http://vlm1.uta.edu/~athitsos/asl_lexicon/\" target = \"_blank\"> aqui </a>. <br><br>\n",
    "    </p>\n",
    "    <a href = \"http://facundoq.github.io/unlp/sign_language_datasets/index.html\" target = \"_blank\">\n",
    "        <li>Sign language datasets</li>\n",
    "    </a>\n",
    "     <p>\n",
    "         Basicamente é um site que compila vários datasets de sign language, entretanto a maioria estão em outros idiomas, o único em inglês é o ASLLVD. Entretanto. Pode haver algo útil. <br><br>\n",
    "      </p>\n",
    "    <a href = \"https://archive.ics.uci.edu/ml/datasets/Libras+Movement\" target = \"_blank\">\n",
    "        <li>Libras Movement Data Set</li>\n",
    "    </a>\n",
    "    <p>\n",
    "        As principais informações da base de dados se encontra no próprio link, foi a única base de dados que encontrei em libras. O vídeo está no formato .data e foi feito pela universidade de São Paulo(USP)\n",
    "     </p>\n",
    "</lu>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
